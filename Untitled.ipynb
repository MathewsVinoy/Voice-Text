{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a708fb2-c2f8-4c62-8e89-e87a0913baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b31627a0-5f18-4fd9-8435-5d5d2f901756",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCHS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e6d56ad-6862-4180-b1e8-732f1dd4acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path(\"data/other.tsv\")\n",
    "audio_path = \"data/train\"\n",
    "data = pd.read_csv(csv_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09cad231-db82-46c5-b8e5-f5ba9a8d4307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accents</th>\n",
       "      <th>variant</th>\n",
       "      <th>locale</th>\n",
       "      <th>segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>420bc8680e03882c43c80e6307ad2a869c892bd90b4388...</td>\n",
       "      <td>common_voice_en_38487408.mp3</td>\n",
       "      <td>His father was undersecretary of the Home Depa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           client_id  \\\n",
       "0  420bc8680e03882c43c80e6307ad2a869c892bd90b4388...   \n",
       "\n",
       "                           path  \\\n",
       "0  common_voice_en_38487408.mp3   \n",
       "\n",
       "                                            sentence  up_votes  down_votes  \\\n",
       "0  His father was undersecretary of the Home Depa...         0           0   \n",
       "\n",
       "   age gender accents  variant locale  segment  \n",
       "0  NaN    NaN     NaN      NaN     en      NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b076b526-bc38-4fde-9b94-eced6d658513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    common_voice_en_38487408.mp3\n",
       "1    common_voice_en_38487409.mp3\n",
       "2    common_voice_en_38487410.mp3\n",
       "3    common_voice_en_38487411.mp3\n",
       "4    common_voice_en_38487412.mp3\n",
       "Name: path, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['path'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c94f545-87e9-40c4-8171-58b47577c049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    His father was undersecretary of the Home Depa...\n",
       "1    The Board's headquarters include a museum of b...\n",
       "2              It was followed by a sequel, Hot Shots!\n",
       "3    One round hit a mine inside, and the machine-g...\n",
       "4    Diamond was educated at Leeds Grammar School a...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentence'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1477b226-db7e-49cb-af0b-a63f6d22974d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/mathews/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "all_tokens = []\n",
    "for text in data['sentence'].astype(str).fillna(''):\n",
    "    tokens = word_tokenize(text)\n",
    "    all_tokens.extend(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a76513cb-8869-4969-a29c-187ec6c0ae16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53906"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = list(set(all_tokens))\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9442a029-1a05-42c1-b0c6-1f8598322b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([51685, 20597,  8212, ..., 29802,  8551,   924])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(tokens)\n",
    "integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02a3d358-b452-4d5a-b6cb-ab48910cbcf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['train', 'Rican', 'Eugenie', ..., 'boosters', 'Faulkes',\n",
       "       'Alspaugh'], dtype='<U128')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_tokens = label_encoder.inverse_transform(integer_encoded)\n",
    "decoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74434cb5-6000-4572-a829-5d331b77f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_num = {char: idx for idx, char in enumerate(set(tokens))}\n",
    "num_to_words = {idx: char for idx, char in enumerate(set(tokens))}\n",
    "\n",
    "encode = lambda s: [words_to_num[c] for c in word_tokenize(s)]\n",
    "def encode2(l):\n",
    "\n",
    "    l=str(l)\n",
    "    value = [words_to_num[c] for c in word_tokenize(l)]\n",
    "    while len(value) <82:\n",
    "        value.append(0)\n",
    "    return torch.tensor(value, dtype=torch.float32)\n",
    "\n",
    "    \n",
    "def decoder(l):\n",
    "    return ' '.join([num_to_words.get(i,'<UNK>') for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae173f55-0790-43e4-9b95-baa47dedf923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['path'], data['sentence'], test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaa0077a-0a16-4d62-bc92-6bc7651a43b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('common_voice_en_38487408.mp3',\n",
       " 'His father was undersecretary of the Home Department of the government of Maharashtra.')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0],y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "062282c2-b4b4-459c-ade0-329e40f47d53",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "libtorch_cuda.so: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchaudio\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mT\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Fallback for MP3\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torchaudio/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dropping_io_support, dropping_class_io_support\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize extension and backend first\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa  # usort: skip\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_backend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa  # usort: skip\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     AudioMetaData \u001b[38;5;28;01mas\u001b[39;00m _AudioMetaData,\n\u001b[1;32m      7\u001b[0m     get_audio_backend \u001b[38;5;28;01mas\u001b[39;00m _get_audio_backend,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     set_audio_backend \u001b[38;5;28;01mas\u001b[39;00m _set_audio_backend,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_torchcodec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_with_torchcodec, save_with_torchcodec\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torchaudio/_extension/__init__.py:38\u001b[0m\n\u001b[1;32m     36\u001b[0m _IS_ALIGN_AVAILABLE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _IS_TORCHAUDIO_EXT_AVAILABLE:\n\u001b[0;32m---> 38\u001b[0m     _load_lib(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibtorchaudio\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_torchaudio\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     _check_cuda_version()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torchaudio/_extension/utils.py:60\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mload_library(path)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/_ops.py:1478\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1473\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m   1475\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m   1476\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m-> 1478\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mCDLL(path)\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/ctypes/__init__.py:390\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m _dlopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, mode)\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: libtorch_cuda.so: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "# Fallback for MP3\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DatasetsCustom(Dataset):\n",
    "    def __init__(self, X, y, path, sample_rate=16000, n_mfcc=40):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (list or pd.Series): list of audio file names\n",
    "            y (list or pd.Series): list of labels (same length as X)\n",
    "            path (str): directory containing audio files\n",
    "        \"\"\"\n",
    "        self.X = X.reset_index(drop=True)\n",
    "        self.y = y.reset_index(drop=True)\n",
    "        self.audio_path = path\n",
    "        self.sample_rate = sample_rate\n",
    "        self.mfcc_transform = T.MFCC(\n",
    "            sample_rate=sample_rate,\n",
    "            n_mfcc=n_mfcc,\n",
    "            melkwargs={\"n_fft\": 2048, \"hop_length\": 512}\n",
    "        )\n",
    "\n",
    "    def load_audio(self, file_name):\n",
    "        audio_path = os.path.join(self.audio_path, file_name)\n",
    "\n",
    "        try:\n",
    "            # Try with torchaudio\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "        except Exception as e:\n",
    "            # If torchaudio fails (e.g. MP3 not supported), fallback to pydub\n",
    "            audio = AudioSegment.from_file(audio_path, format=\"mp3\")\n",
    "            samples = np.array(audio.get_array_of_samples()).astype(np.float32)\n",
    "\n",
    "            # stereo -> mono\n",
    "            if audio.channels > 1:\n",
    "                samples = samples.reshape((-1, audio.channels))\n",
    "                samples = samples.mean(axis=1)\n",
    "\n",
    "            waveform = torch.tensor(samples).unsqueeze(0)  # shape (1, time)\n",
    "            sr = audio.frame_rate\n",
    "\n",
    "        # Resample if needed\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = T.Resample(sr, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # Convert to MFCC -> (n_mfcc, time)\n",
    "        mfcc = self.mfcc_transform(waveform)\n",
    "\n",
    "        # Change shape to (time, n_mfcc) for RNN input\n",
    "        mfcc = mfcc.squeeze(0).transpose(0, 1)\n",
    "        return mfcc\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.X.iloc[idx]\n",
    "        label = self.y.iloc[idx]\n",
    "        mfcc = self.load_audio(file_name)\n",
    "        return mfcc, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea247b8-d11d-4116-9a38-c4bfe4c9b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = DatasetsCustom(X=X_train, y=y_train, path=audio_path)\n",
    "test_datasets  = DatasetsCustom(X=X_test, y=y_test, path=audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3612489b-c354-4ac5-b789-1e2fb8a3579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_datasets, batch_size=32, num_workers=0, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_datasets, batch_size=32, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db1fde7-626c-4d24-af1e-00cef30d0f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AudioModel(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int, num_layers: int = 2, bidirectional: bool = True, dropout: float = 0.3):\n",
    "        super(AudioModel, self).__init__()\n",
    "\n",
    "        # LSTM backbone\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_shape,   # n_mfcc from your dataset (e.g., 40)\n",
    "            hidden_size=hidden_units,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        # Fully connected classifier\n",
    "        rnn_out = hidden_units * (2 if bidirectional else 1)\n",
    "        self.fc = nn.Linear(rnn_out, output_shape)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: [B, T, F] where\n",
    "           B = batch size\n",
    "           T = time steps\n",
    "           F = n_mfcc (input_shape)n_mfcc = 40                 # must match dataset MFCC config\n",
    "hidden_units = 128\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "model = AudioModel(input_shape=n_mfcc, hidden_units=hidden_units, output_shape=num_classes).to(device)\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # LSTM forward\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "        # Take the last timestep output\n",
    "        lstm_out_last = lstm_out[:, -1, :]   # [B, hidden*dir]\n",
    "\n",
    "        # Classification\n",
    "        output = self.fc(lstm_out_last)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e507966-98b1-4890-b332-79751e159d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mfcc = 40                 # must match dataset MFCC config\n",
    "hidden_units = 128\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "model = AudioModel(input_shape=n_mfcc, hidden_units=hidden_units, output_shape=num_classes).to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bace0ac-1b98-49ca-8d68-ace55a1bdce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        inputs, labels = batch\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, labels)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for batch in test_dataloader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch}, train loss {train_loss / len(train_dataloader)}, test loss {test_loss / len(test_dataloader)}\")\n",
    "        \n",
    "    MODEL_PATH = Path(\"models\")\n",
    "    MODEL_NAME = \"model_train.pth\"\n",
    "    MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "    \n",
    "    \n",
    "    MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    #Save\n",
    "    torch.save(obj=model.state_dict(),f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f15630a-da04-43b1-82c9-267abba7b690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f2ee39-2505-4a7f-ad7e-257007f65857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481402b0-9265-4862-9f8a-7734365fc35e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
